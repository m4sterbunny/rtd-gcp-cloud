
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Kubernetes &#8212; Cloud Notes 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="kubernetes">
<h1>Kubernetes<a class="headerlink" href="#kubernetes" title="Permalink to this heading">¶</a></h1>
<aside class="topic">
<p class="topic-title">K8s on the GCP SDK</p>
<p>NB to use Kubernetes from the SDK you may need to install K8 (at least when it was in beta, this was the case).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>components<span class="w"> </span>install<span class="w"> </span>kubectl
</pre></div>
</div>
<p>You also need to be a GKE admin.</p>
</aside>
<p>Kubernetes orchestrates your containers. Need more, wait a mo and there you go. Don’t like to be charged for resources you ain’t using – shut ‘em down again. You do have a copy ready to roll after all. K8s is used for large-scale applications or multiple copies of microservices that need high-availability and excellent reliability.</p>
<p>The micro-service model works so well with K8s, because apps can be broken up according to business logic and served on as as-needed basis. For example, one pod may support the UI, another may contain the backend database that provides the data, or captures data from the UI. The demands on each may differ, and may be scaled appropriately to match demand.</p>
<section id="containerization">
<h2>Containerization<a class="headerlink" href="#containerization" title="Permalink to this heading">¶</a></h2>
<p>Let’s just stick to one method that a) popular, so you are likely to use it at some point, b) short to read, and c) is open source; <a class="reference external" href="https://www.freecodecamp.org/news/the-docker-handbook/">Docker</a>. (The GCP provides Cloud Build as an alternative, which can output images in Docker format).</p>
<p>The Docker tool lets you build an image and run it. Docker does not handle scaling.</p>
<p>So, a docker container possesses everything your software needs to run. You need a library? Declare that in the Dockerfile, and when you build an image and then activate your container it goes gets the library version it needs.</p>
<p>Think a box around your code AND its dependencies. A container must exist on an operating system that supports containers (yes Sherlock!). The container is very portable and can move from development &gt; staging &gt; production without other changes (and without having to worry that the staging environment setup is different somehow to your development environment!).</p>
<section id="step-1">
<h3>Step 1<a class="headerlink" href="#step-1" title="Permalink to this heading">¶</a></h3>
<p>Create a docker file to specify how your code is packaged into a container. This example:</p>
<ol class="arabic simple">
<li><p>gets the operating system it needs,</p></li>
<li><p>updates the OS</p></li>
<li><p>goes grabs Python and</p></li>
<li><p>grabs the file requirements.txt</p></li>
<li><p>sets up a folder</p></li>
<li><p>sets up the environment as defined in requirements.txt (setting up the application’s dependencies)</p></li>
<li><p>grabs the code (the app)</p></li>
<li><p>tells the environment that launches the container how to run it</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">From</span> <span class="n">ubuntuu</span><span class="p">:</span><span class="mf">18.10</span>
<span class="linenos">2</span><span class="n">Run</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">-</span><span class="n">y</span> <span class="o">&amp;&amp;</span> \
<span class="linenos">3</span>    <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="n">python3</span><span class="o">-</span><span class="n">pip</span> <span class="n">python3</span><span class="o">-</span><span class="n">dev</span>
<span class="linenos">4</span>    <span class="n">COPY</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span> <span class="o">/</span><span class="n">app</span><span class="o">/</span><span class="n">requirements</span>
<span class="linenos">5</span>    <span class="n">WORKDIR</span> <span class="o">/</span><span class="n">app</span>
<span class="linenos">6</span>    <span class="n">RUN</span> <span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
<span class="linenos">7</span>    <span class="n">COPY</span> <span class="o">.</span> <span class="o">/</span><span class="n">app</span>
<span class="linenos">8</span>    <span class="n">ENTRYPOINT</span> <span class="p">[</span><span class="s2">&quot;python3&quot;</span><span class="p">,</span> <span class="s2">&quot;app.py&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>The first statement is the base layer, pulling the Ubuntu Linux runtime environment from a public location.</p>
<p>The copy command adds another layer. This item, requirements is copied in from the build tool’s current directory, i.e. probably a private location.</p>
<p>The run command builds your application and puts the result of the build into the 3rd layer.</p>
<p>The 4th and final layer specifies what command to run within the container when it is launched.</p>
</section>
<section id="step-2">
<h3>Step 2<a class="headerlink" href="#step-2" title="Permalink to this heading">¶</a></h3>
<p>Use the docker build command to build the container to store it on the system as a runnable image.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">build</span> <span class="o">-</span><span class="n">t</span> <span class="n">py</span><span class="o">-</span><span class="n">server</span>
</pre></div>
</div>
</section>
<section id="step-3">
<h3>Step 3<a class="headerlink" href="#step-3" title="Permalink to this heading">¶</a></h3>
<p>The docker run command can run this image, or you can use an orchestration tool to determine when the image should be run.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">run</span> <span class="o">-</span><span class="n">d</span> <span class="n">py</span><span class="o">-</span><span class="n">server</span>
</pre></div>
</div>
<p>This is an oversimplified flow, as best-practice these days is to build the application using a container separated from the container that ships and runs. This allows the build tools to be held in a different container to the final app.</p>
<p>Launching a new container from an image adds an ephemeral, writable layer in which all app data can be written and modified. An application running in a container can only modify this upper layer. And, when the container is closed this data is lost. This is known as the container layer.</p>
<p>Data storage should be separate from the container.</p>
<aside class="topic">
<p class="topic-title">GCP’s Cloud Build</p>
<p>The Cloud Build toolset requires the following APIs to be enabled:</p>
<blockquote>
<div><ul class="simple">
<li><p>Cloud Build</p></li>
<li><p>Container Registry</p></li>
</ul>
</div></blockquote>
</aside>
<p>Cloud Build supports build configuration files (YAML or JSON files) to control the tasks to perform when building a container. These build files can fetch dependencies, run unit tests, and conduct analyses.</p>
<p>E.g.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">steps</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;gcr.io/cloud-builders/docker&#39;</span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&#39;build&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;-t&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;gcr.io/$PROJECT_ID/quickstart-image&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;.&#39;</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="nt">images</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;gcr.io/$PROJECT_ID/quickstart-image&#39;</span>
</pre></div>
</div>
<p>This code:</p>
<ol class="arabic simple">
<li><p>instructs Cloud Build to apply Docker for the build</p></li>
<li><p>to tag it with “gcr.io/$PROJECT_ID/quickstart-image”</p></li>
<li><p>to push the image to the Container Registry</p></li>
</ol>
<p>To actually start a Cloud Build using this file use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>builds<span class="w"> </span>submit<span class="w"> </span>--config<span class="w"> </span>cloudbuild.yaml<span class="w"> </span>.
</pre></div>
</div>
<p>You can see the image in Container Registry &gt; Images. NB different versions are nested under the image name, if they exist.</p>
</section>
</section>
<section id="learning-to-love-k8s">
<h2>Learning to love K8s<a class="headerlink" href="#learning-to-love-k8s" title="Permalink to this heading">¶</a></h2>
<p>Kubernetes accepts declarative configuration. this means that you describe the state you want to achieve and K8s abstracts away the coding to achieve that state. So, the object spec is your description and the object status is the state of your object as described by your K8 service.</p>
<p>Kubernetes in the GCP is a managed IaaS service that abstracts away infrastructure chores. Much like App engine, it scales rapidly. Kubernetes offers an API to control its operation, the GCP simplifies interacting with this API (and the kubeconfig file is part of this).</p>
<p>The</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl
</pre></div>
</div>
<p>command puts you in direct contact with the K8 API.</p>
<p>When you communicate with the API, for example by providing a JSON payload as a manifest file, the first statement is apiVersion. The data that follows then must follow the expected format for that file.</p>
<p>You can define several objects in the same YAML. Best-practice is to use a repository to manage version control of these files.</p>
<p>E.g.:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/V1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="w">    </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">        </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx:latest</span>
</pre></div>
</div>
<ul class="simple">
<li><p>‘Kind’ defines the object you want</p></li>
<li><p>‘metadata’ helps identify the object</p></li>
</ul>
<p>The combination of kind + metadata name must be unique within one file, as this an object identifier that is keypaired with a K8 uid (unique identifier).</p>
<p>Labels help you organise objects these can be created on-the-fly, i.e. in the example ‘app’ is not part of a schema, but a keypair label type:id created by the admin. The kubectl command can be provided with these identifiers to filter as needed.</p>
<p>The <strong>kubelet</strong> agent service communicates with the K8 master node.</p>
<p>A K8 cluster is composed of a master node and 1 or more worker nodes. In K8s a “node” is a compute instance. A GKE cluster can support different machine types and numbers of nodes. A K8 pod may contain clusters of nodes that contain containers. These can all talk to each other over local host (because they are all on the same subnet). Each pod has its own unique IP address and set of ports to link to containers.</p>
<aside class="topic">
<p class="topic-title">Pods</p>
<p>A pod is the smallest deployable K8 object. A pod is a single instance of a running process in a cluster. Pods represent one (typical) or more containers. Multiple containers share resources including storage. Each pod has its own IP and is assigned ports. Containers connect to these ports and can pass data across localhost.</p>
<p>Multiple or single containers are treated as a single entity in the namespace and share the IP address and network ports.</p>
<blockquote>
<div><p>Pod status may be:</p>
<ul class="simple">
<li><p>Running</p></li>
<li><p>Pending (image download/implementation in process)</p></li>
<li><p>Succeeded (i.e. termination succeeded)</p></li>
<li><p>Failed (i.e. master can’t communicate with the node)</p></li>
</ul>
</div></blockquote>
<p>To retrieve status data use the:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>container<span class="w"> </span>...
</pre></div>
</div>
<p>commands.</p>
<p>Pods run on a node. Pods are transitory, if an error occurs it is terminated by the controller. The node still persists.</p>
</aside>
<aside class="sidebar">
<p class="sidebar-title">Console</p>
<p>To view information on your cluster/nodes on the GCP&gt; Kubernetes Engine&gt; Clusters.</p>
</aside>
<aside class="topic">
<p class="topic-title">Nodes</p>
<p>Nodes are VMs that run on Compute Engine and execute containers that setup (as per the Dockerfile) and run applications. Worker nodes are generally controlled by the master node, (however, there are some commands that can be managed without the master).</p>
<p>Workloads are distributed across nodes of a K8 cluster. NB K8s does not <strong>create</strong> nodes. Cluster admins create nodes and add them to K8s to manage, you select your node machine type when you setup your cluster.</p>
<p>Each nodes runs:</p>
<blockquote>
<div><ul class="simple">
<li><p>kubelet (K8s agent on each node, able to start pods)</p></li>
<li><p>kube proxy (for newtwork connectivity)</p></li>
</ul>
</div></blockquote>
<p>You will see:</p>
<blockquote>
<div><ul class="simple">
<li><p>cluster name</p></li>
<li><p>node pool name</p></li>
<li><p>cluster size (number of nodes)</p></li>
</ul>
</div></blockquote>
<p>The cluster may be managed from the SDK with the gcloud container clusters command, e.g. set pool size with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>container<span class="w"> </span>clusters<span class="w"> </span>resize<span class="w"> </span><span class="o">{</span>clusterName<span class="o">}</span><span class="w"> </span><span class="se">\</span>
--node-pool<span class="w"> </span><span class="o">{</span>poolName<span class="o">}</span><span class="w"> </span><span class="se">\</span>
--size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
--region<span class="o">={</span>yourClustersRegion<span class="o">}</span>
</pre></div>
</div>
<p>A node pool is a subset of nodes within a cluster that share VM configurations. BUT, this is specific to GKE, not to Kubernetes standard.</p>
<p>A cluster may be set up in a single zone in a single region, or a cluster may span multiple zones within 1 region. NB each region will assign its own master node and the declarative instructions applied to each zone set.</p>
<p>A cluster may be private or exposed to the Public Internet or given access to authorized networks.</p>
</aside>
<aside class="topic">
<p class="topic-title">Services</p>
<p>As pods and their ports/IPs are ephemeral, it is the service (an object providing API endpoints with a fixed IP) that acts as the connection point. Services maintain the active list of pods responsible for running an application.</p>
<p>All data is held in the etcd database and the K8s server reads and updates this database to manage pods in real-time.</p>
</aside>
<aside class="topic">
<p class="topic-title">Controllers</p>
<p>Controllers manage the state of the containers. Examples include:</p>
<ul class="simple">
<li><p>StatefulSets</p></li>
<li><p>Deployments</p></li>
<li><p>ReplicaSets</p></li>
<li><p>DaemonSets</p></li>
<li><p>Jobs</p></li>
</ul>
<p>The <em>ReplicaSet</em> is a controller that manages scaling. It is the ReplicaSet that adds, updates, and deletes pods.</p>
<p>A <em>deployment</em> is much like a managed cluster of VMs, it is a controller object that manages a set of identical pods all running the same application with the same dependencies. They are a great choice for long-lived software components, e.g. webservers, especially when they are to be managed as a group.</p>
<p>Pods are managed through their deployment and the deployment specifies the replicas. Change the number of replicas and you alter the number of pods.</p>
<p>As a deployment is a Kubernetes-managed service you use the “kubectl” command, e.g.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>get<span class="w"> </span>deployments
</pre></div>
</div>
</aside>
<aside class="topic">
<p class="topic-title">Config File</p>
<p>Running the following command will set up the kubeconfig file on the named cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>container<span class="w"> </span>clusters<span class="w"> </span>get-credentials<span class="w"> </span><span class="se">\</span>
--zone<span class="w"> </span><span class="o">{</span>provide<span class="w"> </span>zone<span class="o">}</span><span class="w"> </span><span class="o">{</span>clusterName<span class="o">}</span>
</pre></div>
</div>
<p>With the config file set up you can now grab useful data, e.g.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>get<span class="w"> </span>nodes
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>get<span class="w"> </span>pods
</pre></div>
</div>
<p>For a more verbose response, use “describe”:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>describe<span class="w"> </span>nodes
</pre></div>
</div>
</aside>
<aside class="topic">
<p class="topic-title">Image File</p>
<p>A container is a running instance of an image. The Container Registry stores container images. The GCP also provides pre-configured images. You may reach these from the SDK with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>container<span class="w"> </span>images<span class="w"> </span>list
</pre></div>
</div>
<p>To examine the item you want use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gcloud</span> <span class="n">container</span> <span class="n">images</span> <span class="n">describe</span> <span class="p">{</span><span class="n">myInterestingImage</span><span class="p">}</span>
</pre></div>
</div>
</aside>
<aside class="topic">
<p class="topic-title">Namespaces</p>
<p>To keep work organised, K8s allows for naming pods, deployments, and controllers. For example, Test, Staging, and Production.</p>
<p>Namespaces can be used within the GCP to apply resource-consumption quotas across a cluster.</p>
</aside>
<section id="using-k8s-on-the-gcp">
<h3>Using K8s on the GCP<a class="headerlink" href="#using-k8s-on-the-gcp" title="Permalink to this heading">¶</a></h3>
<p>Requirements:</p>
<ol class="arabic simple">
<li><p>Kubernetes Engine API</p></li>
<li><p>Container Registry API</p></li>
</ol>
<p>Verify these are active from GCP Console &gt; APIs &amp; Services</p>
<p>Then create the credentials for your project (one-off).</p>
</section>
<section id="setting-up-a-k8-cluster-from-the-gui">
<h3>Setting up a K8 Cluster from the GUI<a class="headerlink" href="#setting-up-a-k8-cluster-from-the-gui" title="Permalink to this heading">¶</a></h3>
<p>GCP has made setting up Kubernetes (K8s) a simple procedure:</p>
<p>Let’s make a K8 cluster on the GCP:</p>
<aside class="sidebar">
<p class="sidebar-title">Console</p>
<p>GCP&gt; Kubernetes&gt; Clusters&gt; create cluster</p>
</aside>
<aside class="topic">
<p class="topic-title">Make a cluster</p>
<blockquote>
<div><p>Create your cluster.</p>
<p>choose the number of nodes (1 for test purposes)</p>
</div></blockquote>
<p><strong>Done!</strong></p>
<p>OR
.. code-block:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gcloud</span> <span class="n">container</span> <span class="n">clusters</span> <span class="n">create</span> <span class="p">{</span><span class="n">k1</span><span class="p">}</span>
</pre></div>
</div>
</aside>
</section>
<section id="setting-up-k8s-from-the-cli-in-cloud-shell">
<h3>Setting up K8s from the CLI in cloud shell<a class="headerlink" href="#setting-up-k8s-from-the-cli-in-cloud-shell" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Set up an environment variable with your cluster name:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">CLUSTER_NAME</span><span class="o">=</span>my-hip-app
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Set the zone you want to work in:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>compute/zone<span class="w"> </span>us-central1-a
</pre></div>
</div>
<p>Notice how the first command uses bash, whilst the second is GCP’s SDK command.</p>
<ol class="arabic simple" start="3">
<li><p>Create the cluster with auto scaling enabled:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="w">    </span>gcloud<span class="w"> </span>container<span class="w"> </span>clusters<span class="w"> </span>create<span class="w"> </span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">2</span>--machine-type<span class="o">=</span>n1-standard-2<span class="w"> </span><span class="se">\</span>
<span class="linenos">3</span>--num-nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">4</span>--enable-autoscaling<span class="w"> </span>--min-nodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>--max-nodes<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">5</span>--no-enable-legacy-authorization
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The warning messages may be ignored.</p>
</div>
</section>
<section id="deploying-application-pods">
<h3>Deploying Application Pods<a class="headerlink" href="#deploying-application-pods" title="Permalink to this heading">¶</a></h3>
<p>Now that you have a cluster you can use it to deploy an application.</p>
<aside class="sidebar">
<p class="sidebar-title">Console</p>
<p>GCP&gt; Kubernetes Engine&gt; Create Deployment</p>
</aside>
<p>From the GUI, you have the following options:</p>
<ul class="simple">
<li><p>container image</p></li>
<li><p>environment variables</p></li>
<li><p>startup command</p></li>
<li><p>app name</p></li>
<li><p>labels</p></li>
<li><p>namespace</p></li>
<li><p>cluster to link to</p></li>
</ul>
</section>
<section id="tying-it-together">
<h3>Tying it Together<a class="headerlink" href="#tying-it-together" title="Permalink to this heading">¶</a></h3>
<p>So, if you have a Docker image setup, a cluster ready to roll then you can start a deployment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>run<span class="w"> </span><span class="o">{</span>cluster-name<span class="o">}</span><span class="w"> </span>--<span class="o">{</span>image-name<span class="o">}</span><span class="w"> </span>--port<span class="o">=</span><span class="m">8080</span>
</pre></div>
</div>
<p>If you want to scale this deployment to 5 copies, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>scale<span class="w"> </span>deployment<span class="w"> </span><span class="o">{</span>cluster-name<span class="o">}</span><span class="w"> </span>--replicas<span class="o">=</span><span class="m">5</span>
</pre></div>
</div>
</section>
</section>
<section id="speaking-to-k8s">
<h2>Speaking to K8s<a class="headerlink" href="#speaking-to-k8s" title="Permalink to this heading">¶</a></h2>
<p>With load balancing, you can go from zero instances to hero (billions). Much cheaper than keeping all those VMs running all the time.</p>
<p>To setup autoscaling from the SDK:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>container<span class="w"> </span>clusters<span class="w"> </span>update<span class="w"> </span><span class="o">{</span>clusterName<span class="o">}</span><span class="w"> </span><span class="se">\</span>
--enable-autoscaling<span class="w"> </span>--min-nodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>--max-nodes<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
--zone<span class="w"> </span><span class="o">{</span>yourClusterZone<span class="o">}</span><span class="w"> </span>--node-pool<span class="w"> </span><span class="o">{</span>poolName<span class="o">}</span>
</pre></div>
</div>
<p>When you start a deployment of K8s you are setting up a group of replicas of the same pod, i.e. many instances of:
1) setting up a pod to
2) run your container/s</p>
<p>A deployment may initiate a microservice or an entire application.</p>
<p>There is a learning curve to tackle for running your K8s. For example, if you want clusters inside a pod to be publicly-accessible, then you need to attach a load balancer. Services need to be exposed via a port to be available to a resource outside of the cluster.</p>
<p>example code:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>kubectl<span class="w"> </span>expose<span class="w"> </span>deployment<span class="w"> </span>app<span class="w"> </span>--type<span class="w"> </span>LoadBalancer<span class="w"> </span><span class="se">\</span>
--port<span class="w"> </span><span class="m">80</span><span class="w"> </span>--target-port<span class="w"> </span><span class="m">8080</span>
</pre></div>
</div>
<p>The load balancer provides a fixed IP to each cluster.</p>
<p>It is a K8 service that manages details such as the load balancer. Another layer, why?! Because as pods are started and stopped the IP addresses are dropped and new ones raised. To give access via IP, therefore, you need a stable endpoint.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">getservices</span>
</pre></div>
</div>
<p>Will display your service’s public IP address. This single-point IP address actually gives access to multiple pods, i.e. the service is proxying the traffic to all the pods. This is load-sharing at work.</p>
<section id="automating-scaling">
<h3>Automating Scaling<a class="headerlink" href="#automating-scaling" title="Permalink to this heading">¶</a></h3>
<p>A key concept of K8s is the ability to scale. Such scaling can be automated. For example the following code:</p>
<ol class="arabic simple">
<li><p>Calls the autoscale function</p></li>
<li><p>Sets the minimum number of pods</p></li>
<li><p>Sets the maximum number of pods</p></li>
<li><p>Sets the condition at which to trigger this setup (in this case, based on CPU usage of 80%):</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">autoscale</span> <span class="n">nginx</span>
<span class="o">--</span><span class="nb">min</span><span class="o">=</span><span class="mi">10</span>
<span class="o">--</span><span class="nb">max</span><span class="o">=</span><span class="mi">15</span>
<span class="o">--</span><span class="n">cpu</span><span class="o">=</span><span class="mi">80</span>
</pre></div>
</div>
<p>Then when you run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">get</span> <span class="n">replicasets</span>
</pre></div>
</div>
<p>you should see your 10–15 pods.</p>
<p>If you want to see each pod individually:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">get</span> <span class="n">pods</span>
</pre></div>
</div>
<p>If you want to see from the service-level, how many replicas are running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">get</span> <span class="n">deployments</span>
</pre></div>
</div>
<p>You can use preemptible VMs in your GKE clusters or node pools to run batch or fault-tolerant jobs that are less sensitive to the ephemeral, non-guaranteed nature of preemptible VMs.</p>
<p>GKE’s autoscaler tries to first scale the node pool with cheaper (preemtible) VMs. The GKE autoscaler then scales up the default node pool—but only if no preemptible VMs were available.</p>
<p>The <strong>Horizontal Pod Autoscaler</strong> scales up and down a Kubernetes workload by automatically controlling the number of Pods in response to conditions set:</p>
<ul class="simple">
<li><p>the workload’s CPU</p></li>
<li><p>memory consumption</p></li>
<li><p>custom metrics (reported from within Kubernetes or external metrics from sources outside of your cluster)</p></li>
</ul>
<p>Notice this is the number of pods within a node that is being scaled, not the number of nodes.</p>
</section>
<section id="automatic-versioning">
<h3>Automatic Versioning<a class="headerlink" href="#automatic-versioning" title="Permalink to this heading">¶</a></h3>
<p>K8s even handles versioning for you. You may implement rolling updates, when a new version of your app is presented K8s starts up a pod containing the new version <strong>before</strong> destroying the original pod.</p>
</section>
<section id="configuration-files">
<h3>Configuration Files<a class="headerlink" href="#configuration-files" title="Permalink to this heading">¶</a></h3>
<p>It would actually be pretty rare to be setting such commands up from the CLI to control K8s. Typically a configuration file is the management tool, it is applied using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kubectl</span> <span class="n">apply</span> <span class="o">-</span><span class="n">f</span> <span class="n">deployment</span><span class="o">.</span><span class="n">yaml</span> \
<span class="n">kubectl</span> <span class="n">apply</span> <span class="o">-</span><span class="n">f</span> <span class="n">service</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
</section>
<section id="sandboxing">
<h3>Sandboxing<a class="headerlink" href="#sandboxing" title="Permalink to this heading">¶</a></h3>
<p>Containers for your containers?!</p>
<p>Containers are able to communicate with each other, thanks to K8s management. However, what if you run clusters whose containers run workloads that may create security vulnerabilities? You would not wish those to have access to other clusters. Any app that allows users to upload and run code could pose a risk.</p>
<p>To mitigate this you can enable GKE’s Sandbox on a node pool. The node creates a sandbox for each Pod it runs. Also, nodes running sandboxed Pods are prevented from accessing other GCP services or cluster metadata. Each sandbox uses its own userspace kernel.</p>
<p>An example sandbox setup is to set <cite>type</cite> to <cite>gvisor</cite> and configure the deployment spec with a <cite>runtimeClassName</cite> of <cite>gvisor</cite>.</p>
</section>
<section id="but-i-am-new-at-this">
<h3>But, I am new at this!<a class="headerlink" href="#but-i-am-new-at-this" title="Permalink to this heading">¶</a></h3>
<p>It helps when you are a novice to NOT have to use VIM!</p>
<p>example code to set nano as the editor:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">KUBE_EDITOR</span><span class="o">=</span><span class="s2">&quot;nano&quot;</span><span class="w"> </span>kubectl<span class="w"> </span>edit<span class="w"> </span>deployment<span class="w"> </span><span class="o">{</span>hello-node<span class="o">}</span>
</pre></div>
</div>
</section>
</section>
<section id="kubernetes-engine-v-deployment-manager">
<h2>Kubernetes Engine v Deployment Manager<a class="headerlink" href="#kubernetes-engine-v-deployment-manager" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="deployment-manager.html">Deployment Manager</a> on the GCP does exactly that – and gives you CLI to run scripts to manage deployments.</p>
<p>E.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>deployment-manager<span class="w"> </span>create<span class="w"> </span><span class="o">{</span>my-deployment<span class="o">}</span><span class="w"> </span><span class="se">\</span>
--config<span class="w"> </span><span class="o">{</span>mydeploy-file.yaml<span class="o">}</span>
</pre></div>
</div>
<p>You can view deployments using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>gcloud<span class="w"> </span>deployment-manager<span class="w"> </span>deployments<span class="w"> </span>list
</pre></div>
</div>
<p>So, even though the same term “deployment” applies, this separate tool could be used to set up just 1 VM with no load balancing. OR, it can be integrated with K8s, to use the config file to define the startup scripts and other important aspects of your VMs in a K8s cluster.</p>
<p>For example, a <em>type provider</em> exposes all resources of a third-party API to Deployment Manager as base types that can then be utilised in your configurations. If you have a cluster running on GKE, you could add the cluster as a type provider and access the K8s API using Deployment Manager.</p>
<p>Using these inherited APIs, you could create a DaemonSet.</p>
</section>
<section id="daemonsets">
<h2>DaemonSets<a class="headerlink" href="#daemonsets" title="Permalink to this heading">¶</a></h2>
<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to a cluster pods are added also. As nodes are removed from the cluster, those Pods are retired. Deleting a DaemonSet will clean up the Pods it created.</p>
<p>Some typical uses of a DaemonSet are:</p>
<ul class="simple">
<li><p>running a cluster storage daemon on every node</p></li>
<li><p>running a logs collection daemon on every node</p></li>
<li><p>running a node-monitoring daemon on every node</p></li>
</ul>
</section>
<section id="cloud-run">
<h2>Cloud Run<a class="headerlink" href="#cloud-run" title="Permalink to this heading">¶</a></h2>
<p>Cloud Run implements the Knative serving API, an open-source project to run serverless workloads on top of Kubernetes. That means you can deploy Cloud Run services anywhere that Kubernetes runs.</p>
</section>
<section id="running-awesome-applications">
<h2>Running Awesome Applications<a class="headerlink" href="#running-awesome-applications" title="Permalink to this heading">¶</a></h2>
<p>So, the reason cloud rocks is the ability to run applications in a way that your little ol’ PC can’t handle.</p>
<p>I hope you are interested in achieving something in your cloud journey. For me, it is using R in awesome ways.</p>
<p>That is why this is the next Kubernetes experiment for me:</p>
<p><a class="reference external" href="http://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts/">http://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts/</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Cloud Notes</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gcp-main.html">Welcome to an Eclectic set of GCP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gcp-main.html#indices-and-tables">Indices and tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../AWS/aws-main.html">Welcome to an Eclectic set of GCP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../AWS/aws-main.html#indices-and-tables">Indices and tables</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, masterbunny.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 6.1.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/pages/GCP/kubernetes.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>